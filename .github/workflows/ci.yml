name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black ruff pytest
    
    - name: Run black
      run: black --check src/ tests/ scripts/ demo/
    
    - name: Run ruff
      run: ruff check src/ tests/ scripts/ demo/

  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.10, 3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run tests
      run: pytest tests/ -v --cov=src --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  inference-test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
    
    - name: Test model inference
      run: |
        python -c "
        import sys
        sys.path.append('src')
        from models import StyleTransferModel
        import torch
        import numpy as np
        
        # Test model creation and inference
        model = StyleTransferModel(model_type='cnn')
        content = torch.randn(1, 1, 128, 100)
        style = torch.randn(1, 1, 128, 100)
        
        with torch.no_grad():
            output = model(content, style)
        
        print(f'Model created successfully: {model.get_model_info()}')
        print(f'Output shape: {output.shape}')
        print('Inference test passed!')
        "
